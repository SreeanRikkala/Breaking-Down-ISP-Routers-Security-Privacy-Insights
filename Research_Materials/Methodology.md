
# Project Methodology: ISP Router Evaluation

## Project Overview
This project evaluates ISP-provided and compatible consumer routers using a quantifiable scoring framework across four core categories: Security, Usability, Performance, and Cost.

## Objectives
- Identify key evaluation criteria based on best practices and industry standards.
- Create a scoring matrix to evaluate routers consistently.
- Present results in a way that empowers consumers to understand their router's strengths and weaknesses.

## Research Standards & Sources
Our methodology is grounded in the following:
- NIST IR 8425A – Cybersecurity Requirements for Consumer-Grade Routers
- CableLabs Gateway Device Security Best Common Practices (CL-GL-GDS-BCP-V01-211007)
- RFC 2544 – Network Performance Testing
- Ookla Speedtest Reports – Crowdsourced performance benchmarks
- SmallNetBuilder, CNET, Reddit, and manufacturer documentation
- BSI TR-03148 Secure Broadband Router 
- TR-124: Functional Requirements for Broadband Residential Gateway Devices
- NIST SP 800-128 – Security-Focused Configuration Management
- System Usability Scale (SUS)
- Heuristic Evaluation (Nielsen)

## Evaluation Framework
We score routers in four categories:

| Category     | Weight |
|--------------|--------|
| Security     | 50%    |
| Usability    | 17%    |
| Performance  | 25%    |
| Cost         | 8%     |

## Weighting Justification
Security was prioritized due to its critical role in protecting home networks and data. Performance and usability were included based on their impact on consumer satisfaction, while cost considerations were included with lower weight to reflect long-term value without compromising security.

## Data Sources
Due to the lack of physical access to devices, all scoring is based on:
- Official ISP and manufacturer documentation
- Public vulnerability databases (e.g., CVE records)
- Third-party benchmark reviews
- User-reported performance from forums (used cautiously)

## Limitations
- Certain internal security features could not be verified (e.g., secure boot)
- Performance metrics were estimated using third-party reports
- Subjective usability feedback was standardized using established heuristics

